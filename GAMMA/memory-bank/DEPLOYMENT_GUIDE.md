# üöÄ DEPLOYMENT GUIDE - Hndasah PM System v3.0 (Gamma)

**Production Deployment | Infrastructure Setup | Monitoring Configuration**
**Status:** Ready for Production | Enterprise-Grade | Scalable

---

## üéØ **DEPLOYMENT OVERVIEW**

### **Infrastructure Requirements**
```
Production Environment:
‚îú‚îÄ‚îÄ Load Balancer (NGINX/Cloudflare)
‚îú‚îÄ‚îÄ Application Servers (5x replicas)
‚îú‚îÄ‚îÄ Database Cluster (PostgreSQL 16)
‚îú‚îÄ‚îÄ Redis Cluster (Caching & Sessions)
‚îú‚îÄ‚îÄ Monitoring Stack (Prometheus/Grafana)
‚îî‚îÄ‚îÄ CDN (Static Assets & Media)
```

### **Performance Targets**
- **API Response Time:** <100ms average
- **Page Load Time:** <2 seconds
- **Concurrent Users:** 1000+ supported
- **Uptime:** 99.999% (five nines)
- **Data Durability:** 99.999999999% (eleven nines)

---

## üê≥ **DOCKER CONTAINERIZATION**

### **Frontend Container**
```dockerfile
# Dockerfile.frontend
FROM node:18-alpine AS builder

# Install dependencies
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

# Build application
COPY . .
RUN npm run build

# Production image
FROM nginx:alpine
COPY --from=builder /app/out /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

### **Backend Container**
```dockerfile
# Dockerfile.backend
FROM python:3.12-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Health checks
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### **Database Container**
```dockerfile
# Dockerfile.postgres
FROM postgres:16

# Custom PostgreSQL configuration
COPY postgresql.conf /etc/postgresql/postgresql.conf
COPY init.sql /docker-entrypoint-initdb.d/

# Health checks
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD pg_isready -U hndasah_pm || exit 1

EXPOSE 5432
```

---

## ‚òÅÔ∏è **CLOUD DEPLOYMENT OPTIONS**

### **AWS Deployment**
```yaml
# AWS CloudFormation template
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Hndasah PM Production Stack'

Resources:
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true

  ECSCluster:
    Type: AWS::ECS::Cluster
    Properties:
      ClusterName: hndasah-pm-cluster

  RDSInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceClass: db.r6g.xlarge
      Engine: postgres
      EngineVersion: '16.0'
      AllocatedStorage: '100'
      DBInstanceIdentifier: hndasah-pm-db
      MasterUsername: !Ref DBUsername
      MasterUserPassword: !Ref DBPassword

  ElastiCacheCluster:
    Type: AWS::ElastiCache::CacheCluster
    Properties:
      CacheNodeType: cache.r6g.large
      Engine: redis
      NumCacheNodes: '2'
      CacheClusterId: hndasah-pm-redis

  ALB:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Type: application
      Scheme: internet-facing
      SecurityGroups:
        - !Ref ALBSecurityGroup
```

### **Google Cloud Platform**
```yaml
# GCP deployment configuration
resources:
- name: hndasah-pm-backend
  type: cloudrun.googleapis.com/v1
  properties:
    location: us-central1
    metadata:
      spec:
        template:
          spec:
            containers:
            - image: gcr.io/hndasah-pm/backend:latest
              env:
              - name: DATABASE_URL
                value: /secrets/database-url
              - name: REDIS_URL
                value: /secrets/redis-url
              resources:
                limits:
                  cpu: '2'
                  memory: '2Gi'

- name: hndasah-pm-db
  type: sqladmin.googleapis.com/v1beta4
  properties:
    databaseVersion: POSTGRES_16
    settings:
      tier: db-custom-4-15360
      diskSizeGb: 100
      backupConfiguration:
        enabled: true
        startTime: 02:00
```

---

## üõ†Ô∏è **INFRASTRUCTURE AS CODE**

### **Terraform Configuration**
```hcl
# main.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC Configuration
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"

  name = "hndasah-pm-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["${var.aws_region}a", "${var.aws_region}b", "${var.aws_region}c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = true
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "hndasah-pm-cluster"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

# RDS PostgreSQL
resource "aws_db_instance" "postgres" {
  identifier             = "hndasah-pm-db"
  engine                 = "postgres"
  engine_version         = "16.0"
  instance_class         = "db.r6g.xlarge"
  allocated_storage      = 100
  storage_type           = "gp3"
  db_name                = "hndasah_pm"
  username               = var.db_username
  password               = var.db_password
  parameter_group_name   = aws_db_parameter_group.postgres.name
  skip_final_snapshot    = true
  multi_az               = true

  backup_retention_period = 7
  backup_window           = "02:00-03:00"
  maintenance_window      = "sun:03:00-sun:04:00"
}
```

---

## üìä **MONITORING & OBSERVABILITY**

### **Prometheus Configuration**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'hndasah-pm-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s

  - job_name: 'hndasah-pm-frontend'
    static_configs:
      - targets: ['frontend:80']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
    scrape_interval: 30s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 30s
```

### **Grafana Dashboards**
```json
// System Overview Dashboard
{
  "dashboard": {
    "title": "Hndasah PM - System Overview",
    "tags": ["hndasah-pm", "system"],
    "timezone": "UTC",
    "panels": [
      {
        "title": "API Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"hndasah-pm-backend\"}[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Active Users",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(active_users)",
            "legendFormat": "Active Users"
          }
        ]
      },
      {
        "title": "Database Connections",
        "type": "gauge",
        "targets": [
          {
            "expr": "pg_stat_activity_count{datname=\"hndasah_pm\"}",
            "legendFormat": "DB Connections"
          }
        ]
      }
    ]
  }
}
```

### **Alert Rules**
```yaml
# alert_rules.yml
groups:
  - name: hndasah_pm_alerts
    rules:
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High API response time"
          description: "95th percentile response time is {{ $value }}s"

      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL database down"
          description: "PostgreSQL has been down for more than 1 minute"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"
```

---

## üîê **SECURITY CONFIGURATION**

### **SSL/TLS Setup**
```nginx
# nginx.conf
server {
    listen 443 ssl http2;
    server_name api.hndasah-pm.com;

    ssl_certificate /etc/ssl/certs/hndasah_pm.crt;
    ssl_certificate_key /etc/ssl/private/hndasah_pm.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384;

    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubdomains";

    location / {
        proxy_pass http://backend:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

### **Secrets Management**
```yaml
# Kubernetes secrets
apiVersion: v1
kind: Secret
metadata:
  name: hndasah-pm-secrets
type: Opaque
data:
  database-url: <base64-encoded-database-url>
  redis-url: <base64-encoded-redis-url>
  jwt-secret: <base64-encoded-jwt-secret>
  whatsapp-token: <base64-encoded-whatsapp-token>
  aws-access-key: <base64-encoded-aws-key>
  aws-secret-key: <base64-encoded-aws-secret>
---
# AWS Secrets Manager
{
  "ARN": "arn:aws:secretsmanager:us-east-1:123456789012:secret:hndasah-pm/prod/database-ABC123",
  "Name": "hndasah-pm/prod/database",
  "SecretString": "{\"username\":\"admin\",\"password\":\"secret\",\"host\":\"prod-db.cluster-abc123.us-east-1.rds.amazonaws.com\"}"
}
```

---

## üìà **PERFORMANCE OPTIMIZATION**

### **CDN Configuration**
```javascript
// Cloudflare Worker for API optimization
addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request))
})

async function handleRequest(request) {
  // API request optimization
  if (request.url.includes('/api/')) {
    // Add caching headers
    const response = await fetch(request)
    const newResponse = new Response(response.body, response)

    // Cache API responses for 5 minutes
    newResponse.headers.set('Cache-Control', 'public, max-age=300')
    newResponse.headers.set('CDN-Cache-Control', 'max-age=300')

    return newResponse
  }

  // Static asset optimization
  if (request.url.match(/\.(css|js|png|jpg|svg)$/)) {
    const response = await fetch(request)
    const newResponse = new Response(response.body, response)

    // Cache static assets for 1 year
    newResponse.headers.set('Cache-Control', 'public, max-age=31536000, immutable')

    return newResponse
  }

  return fetch(request)
}
```

### **Database Performance Tuning**
```sql
-- PostgreSQL performance configuration
-- postgresql.conf optimizations
shared_buffers = '256MB'
effective_cache_size = '1GB'
work_mem = '4MB'
maintenance_work_mem = '64MB'
checkpoint_completion_target = 0.9
wal_buffers = '16MB'
default_statistics_target = 100

-- Connection pooling with PgBouncer
[databases]
hndasah_pm = host=localhost port=5432 dbname=hndasah_pm

[pgbouncer]
listen_port = 6432
listen_addr = 127.0.0.1
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 20
reserve_pool_size = 5
```

---

## üîÑ **BACKUP & DISASTER RECOVERY**

### **Database Backup Strategy**
```bash
#!/bin/bash
# Daily backup script
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups"
DB_NAME="hndasah_pm"

# Create backup
pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME -F c -f $BACKUP_DIR/${DB_NAME}_${DATE}.backup

# Compress backup
gzip $BACKUP_DIR/${DB_NAME}_${DATE}.backup

# Upload to S3
aws s3 cp $BACKUP_DIR/${DB_NAME}_${DATE}.backup.gz s3://hndasah-pm-backups/

# Cleanup old backups (keep last 30 days)
find $BACKUP_DIR -name "*.backup.gz" -mtime +30 -delete

# Verify backup integrity
pg_restore --list $BACKUP_DIR/${DB_NAME}_${DATE}.backup.gz > /dev/null
if [ $? -eq 0 ]; then
    echo "Backup successful: ${DB_NAME}_${DATE}.backup.gz"
else
    echo "Backup verification failed!"
    exit 1
fi
```

### **Disaster Recovery**
```yaml
# Disaster recovery runbook
disaster_recovery:
  rto: "4 hours"        # Recovery Time Objective
  rpo: "15 minutes"     # Recovery Point Objective

  procedures:
    database_failover:
      - Detect primary database failure
      - Promote read replica to primary
      - Update application configuration
      - Verify application functionality
      - Notify stakeholders

    application_failover:
      - Detect application failure via monitoring
      - Scale up remaining instances
      - Route traffic to healthy instances
      - Investigate root cause
      - Implement fix and redeploy

    data_center_failover:
      - Detect data center outage
      - Activate secondary region
      - Update DNS to point to secondary region
      - Scale up secondary region resources
      - Verify full system functionality
```

---

## üöÄ **DEPLOYMENT PIPELINE**

### **CI/CD Pipeline**
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          npm run test --workspace=frontend
          python -m pytest backend/tests/

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Build and push Docker images
        run: |
          docker build -t hndasah/pm-frontend:${{ github.sha }} ./frontend
          docker build -t hndasah/pm-backend:${{ github.sha }} ./backend
          docker push hndasah/pm-frontend:${{ github.sha }}
          docker push hndasah/pm-backend:${{ github.sha }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/hndasah-pm-frontend frontend=hndasah/pm-frontend:${{ github.sha }}
          kubectl set image deployment/hndasah-pm-backend backend=hndasah/pm-backend:${{ github.sha }}
          kubectl rollout status deployment/hndasah-pm-backend
          kubectl rollout status deployment/hndasah-pm-frontend

  e2e-test:
    needs: deploy
    runs-on: ubuntu-latest
    steps:
      - name: Run E2E tests
        run: |
          npm run test:e2e
```

### **Blue-Green Deployment**
```bash
#!/bin/bash
# Blue-green deployment script

# Deploy to green environment
kubectl apply -f k8s/green/
kubectl wait --for=condition=available --timeout=300s deployment/hndasah-pm-backend-green

# Run smoke tests on green
if smoke_test_green; then
    echo "Smoke tests passed, switching traffic to green"

    # Switch ingress to green
    kubectl patch ingress hndasah-pm -p '{"spec":{"rules":[{"host":"api.hndasah-pm.com","http":{"paths":[{"path":"/","pathType":"Prefix","backend":{"service":{"name":"hndasah-pm-backend-green","port":{"number":80}}}}]}}]}}'

    # Wait for traffic to switch
    sleep 60

    # Scale down blue environment
    kubectl scale deployment hndasah-pm-backend-blue --replicas=0

    echo "Blue-green deployment successful"
else
    echo "Smoke tests failed, rolling back"
    kubectl delete -f k8s/green/
    exit 1
fi
```

---

## üìä **COST OPTIMIZATION**

### **AWS Cost Optimization**
```hcl
# Cost-optimized instance types
resource "aws_db_instance" "postgres" {
  instance_class = "db.r6g.xlarge"  # Graviton2 for cost savings
  storage_type   = "gp3"            # Latest generation SSD

  # Reserved instances for steady-state workloads
  reserved_instance_id = aws_db_reserved_instance.postgres.id
}

# Auto-scaling for variable workloads
resource "aws_appautoscaling_target" "ecs_target" {
  max_capacity       = 10
  min_capacity       = 2
  resource_id        = "service/hndasah-pm-cluster/hndasah-pm-service"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

resource "aws_appautoscaling_policy" "ecs_cpu_policy" {
  name               = "cpu-autoscaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.ecs_target.resource_id
  scalable_dimension = aws_appautoscaling_target.ecs_target.scalable_dimension
  service_namespace  = aws_appautoscaling_target.ecs_target.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    target_value = 70.0
  }
}
```

---

## üéØ **GO-LIVE CHECKLIST**

### **Pre-Deployment**
- [ ] **Security Review:** Penetration testing completed
- [ ] **Performance Testing:** Load testing with 2x expected traffic
- [ ] **Data Migration:** Production data seeded and verified
- [ ] **Backup Testing:** Restore procedures tested
- [ ] **Monitoring Setup:** All alerts and dashboards configured
- [ ] **SSL Certificates:** Valid certificates installed
- [ ] **DNS Configuration:** Production domains configured

### **Deployment Day**
- [ ] **Database Migration:** Schema updates applied safely
- [ ] **Application Deployment:** Zero-downtime deployment
- [ ] **Configuration Updates:** Environment variables updated
- [ ] **Cache Warming:** Critical data pre-loaded
- [ ] **Health Checks:** All services reporting healthy
- [ ] **Traffic Verification:** Requests routing correctly

### **Post-Deployment**
- [ ] **Monitoring Validation:** All metrics collecting data
- [ ] **Alert Testing:** Critical alerts functioning
- [ ] **Performance Verification:** Response times within targets
- [ ] **User Acceptance Testing:** Key user journeys validated
- [ ] **Documentation Update:** Runbooks and procedures updated
- [ ] **Team Training:** Operations team trained on new system

---

*Complete production deployment guide for Hndasah PM system with enterprise-grade infrastructure, monitoring, and operational procedures.*
